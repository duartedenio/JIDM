\section{Introduction}
\label{sec:intro}

\color{blue} 

Lately, we witness a flood of data generated by several data-centric applications. 
Some areas of science, for example, are facing a huge increase of data volumes from satellites, telescopes, high-throughput instruments, sensor networks, accelerators, and supercomputers~\citep{BHS09}. 
The generated data are mostly weakly structured, irregular, and incomplete~\citep{Ke+22}; besides they do not follow a predefined schema. 
To store and exchange this kind of data, JSON has become one of the most popular formats~\citep{bourhis2017json}. 
JSON is a lightweight format and its documents are collections of key-value pairs, i.e., it stores data (value) and metadata (key) altogether a given document. 
This structure makes JSON documents be loosely coupled with schemas. 
On the other hand, applications that need to access this kind of data must have some reliable notion of its schema. 

Schemas are essential for data-centric applications, and \citep{Ke+22} show some tasks which schemas can be usuful:

\begin{itemize}

  \item Source Selection: it allows to choose the best data source for what users are looking for,
   
  \item Query Formulation: schemas decribe the content of a data source and so give a better overview of the content to be queried. 
  This is also true when a query must be decopomsed or optimized. 
  Knowing the documents structures makes easier to query multiple datas ource to find the best query decomposition.

  \item Data Indexing: the schema of a data source $\mathcal{D}$ may be a guide to built indexes on $\mathcal{D}$.
  
  \item Query Answering: the schema of a data source $\mathcal{D}$ allows to detemine whether or not a subset of $\mathcal{D}$ contains the answer of a given query.
  
  \item Data integration: the use of schema information to combine data sources is essential to achieve better results.
  
  \item Data Quality Assessment: schemas allow to use metrics to evaluate the quality of a data source, e.g., the completeness of a dataset reagarding its schema.
  
  \item Data Partitioning: when data must be distribute in multiple nodes, schema enable a better partioning plan.
\end{itemize} 

\color{black} 

Given a collection of JSON documents, extracting the latent schema presents a challenge due to its inherently flexible and dynamic nature ~\citep{canovas2013discovering}. JSON allows nested objects, arrays, and various structures with no enforced schema, making it difficult to infer a consistent and accurate schema. 
These complexities and the advantages of using a schema create a necessity for schema extraction approaches. 

Many schema extraction approaches have been proposed based on the importance of a schema ~\citep{frozza2018approach, baazizi2019parametric,abdelhedi2021automatic, klessinger2022extracting}. Each one explores the schema extraction on different faces of the JSON with distinct approaches. All approaches extract the basic JSON types (string, number, boolean, object, and array) but differ on the other JSON features (\textit{e.g.} tagged union, and enumerations). The works of Frozza et al. (2018), Baazizi et al. (2019), and Abdelhedi et al. (2021) focus only on the extraction of the basic types. Namba and Mior (2021), and Spoth et al. (2021) propose complete tools; however, they cannot discover tagged unions and enumerations. Although Klessinger et al. (2022) include tagged unions in the target schema, other types are not considered. 
Despite schema extraction being addressed in the state of the art, we are unaware of any approach that deals with the JSON structure complexity. 

In this paper, we present \textit{JFUSE} (Json FUll Schema Extractor), a novel approach for schema extraction. Our approach maps the JSON fields to vertices and uses edges to map relationships between them (\textit{e.g.}, parenting or sibling). In the graph, we store information about the occurrence of each field and their relationship to facilitate the inference of the schema. Based on the graph, we generate a metamodel representing the schema rules. Our approach can extract information about the basic JSON data types and features like tagged unions, enumeration, data collections, and tuples encoded in arrays.
\color{blue} 
It extends the work presented in \cite{Ba+24}. 
The new contributions are as follows: ($i$) keys constraints are now extracted, ($ii$) new experiments are conducted, and ($iii$)


\color{black}

We validate JFUSE executing two sets of experiments. The first one is to validate the approach against real-world datasets, evaluating the quality of the extracted schema. The second experiment focuses on proving the approach's concept, testing the extraction against a synthetic dataset created based on the different facets of a JSON (basic types, tagged union, tuple, array, metadata, object collection, and enumeration). The results show a concise schema regarding the size of the input collections and a satisfactory execution time. Moreover, the experiments also showed that our approach is scalable. 


The rest of this paper is organized as follows: Section \ref{sec:prelim} reviews the JSON data model and the main schema concepts. Section \ref{sec:RW} presents the related work, highlighting the limitations of the existing approaches. Section ~\ref{sec:SchDisc} details our graph-based methodology, including the definitions of the meta-model. Section ~\ref{sec:ResDisc}  presents the experimental results and discussion. Finally, Section ~\ref{sec:Concl} concludes the paper.

